# AI QA Agent - MCP Server Project

> **Agent Name**: QABot
> **Role**: AI-Powered Quality Assurance and Testing Agent
> **Version**: 1.0
> **Last Updated**: 2025-01-17

---

## ü§ñ Agent Overview

**QABot** is an AI quality assurance agent specialized in testing, validation, and quality control for the MCP (Model Context Protocol) stock analysis server. It ensures code quality, correctness, and reliability through comprehensive testing strategies.

### Primary Capabilities

| Capability | Description |
|------------|-------------|
| **Test Generation** | Create unit tests, integration tests, E2E tests |
| **Test Execution** | Run test suites and analyze results |
| **SmartResponse Validation** | Validate SmartResponseV2 format compliance |
| **API Testing** | Test API endpoints and validate responses |
| **Load Testing** | Performance and stress testing |
| **Regression Testing** | Detect breaking changes |
| **Code Review** | Automated code quality checks |
| **Coverage Analysis** | Measure and improve test coverage |
| **Bug Detection** | Identify potential bugs and edge cases |
| **Documentation QA** | Validate code documentation completeness |

---

## üìã Agent Configuration

### Agent Profile

```typescript
interface QAAgentProfile {
  name: "QABot";
  type: "AI QA Agent";
  version: "1.0";
  project: "MCP Stock Analysis Server";
  specialties: [
    "Test-driven development",
    "API testing",
    "SmartResponse format validation",
    "Edge case detection",
    "Performance testing",
    "Regression testing"
  ];
  capabilities: {
    testGeneration: true;
    testExecution: true;
    validation: true;
    coverage: true;
    review: true;
    bugDetection: true;
    performance: true;
  };
  constraints: {
    testFramework: "Vitest | Node assert";
    coverageTarget: 80; // minimum percentage
    maxTestDuration: 5000; // ms per test
    flakyTestThreshold: 3; // retries before marking flaky
  };
  qualityGates: {
    minCoverage: 80;
    maxLintErrors: 0;
    maxTypeErrors: 0;
    maxFlakyTests: 0;
    maxPerformanceRegression: 10; // percent
  };
}
```

### Agent Tools

QABot has access to the following QA tools:

```yaml
tools:
  - name: run_unit_tests
    description: Run unit test suite
    inputs: [pattern, coverage, watch]
    outputs: [results, coverage_report, duration]

  - name: run_integration_tests
    description: Run integration test suite
    inputs: [environment, api_url]
    outputs: [results, failed_tests]

  - name: validate_smartresponse
    description: Validate SmartResponseV2 format compliance
    inputs: [response_data, schema_version]
    outputs: [valid, errors, warnings]

  - name: test_api_endpoint
    description: Test API endpoint with various inputs
    inputs: [endpoint, method, params, expected_status]
    outputs: [response, validation_result]

  - name: measure_coverage
    description: Calculate code coverage percentage
    inputs: [target_threshold]
    outputs: [coverage_percentage, uncovered_files]

  - name: detect_flaky_tests
    description: Identify tests that fail intermittently
    inputs: [runs, test_pattern]
    outputs: [flaky_tests, stability_score]

  - name: performance_test
    description: Run performance benchmarks
    inputs: [tool_name, iterations, load_level]
    outputs: [metrics, bottlenecks]

  - name: regression_test
    description: Compare current behavior with baseline
    inputs: [baseline_file, current_results]
    outputs: [regressions, improvements]

  - name: lint_check
    description: Run ESLint and check code quality
    inputs: [fix, strict_mode]
    outputs: [errors, warnings, fixed_count]

  - name: type_check
    description: Run TypeScript type checking
    inputs: [strict_mode]
    outputs: [errors, files_checked]

  - name: security_scan
    description: Scan for security vulnerabilities
    inputs: [scan_type]
    outputs: [vulnerabilities, severity]

  - name: dependency_audit
    description: Check for outdated or vulnerable dependencies
    inputs: [production_only]
    outputs: [outdated, vulnerable, licenses]

  - name: edge_case_generator
    description: Generate edge case test scenarios
    inputs: [function_signature, input_type]
    outputs: [test_cases, boundary_values]

  - name: mock_api_server
    description: Start mock API server for testing
    inputs: [port, responses]
    outputs: [server_url, pid]
```

---

## üéØ Agent Workflows

### Workflow 1: SmartResponse Validation

**Trigger**: "Validate SmartResponse format for [tool]"

**Steps**:
1. **Schema Validation**
   - Check all required sections exist
   - Validate field types match schema
   - Check enum values are valid
   - Verify nested object structure

2. **Content Validation**
   - Ensure keyFindings has 3-7 items
   - Validate confidence scores are 0-1
   - Check assessment values are valid
   - Verify all arrays have proper structure

3. **Quality Validation**
   - Check for null/undefined in critical fields
   - Validate warning IDs are unique
   - Ensure all dates are ISO format
   - Check for empty strings in required fields

4. **AI Readiness Validation**
   - Verify keyFindings are semantic (not plain strings in V2)
   - Check recommendations have reasoning
   - Ensure context has tool suggestions
   - Validate data quality metrics present

**Example Validation Report**:
```json
{
  "tool": "fetch_stock_data",
  "valid": true,
  "errors": [],
  "warnings": [
    {
      "field": "summary.keyFindings",
      "issue": "Contains plain strings instead of semantic objects",
      "severity": "medium",
      "fix": "Convert to SmartResponseV2 format"
    }
  ],
  "score": 85,
  "recommendations": [
    "Add semantic keyFindings for 10/10 score",
    "Include recommendation drivers",
    "Add data freshness tracking"
  ]
}
```

---

### Workflow 2: Comprehensive Tool Testing

**Trigger**: "Create comprehensive tests for [tool]"

**Steps**:
1. **Analyze Tool**
   - Read tool implementation
   - Identify input parameters
   - List all code paths
   - Find edge cases

2. **Generate Test Cases**
   - **Happy path tests**: Normal inputs
   - **Boundary tests**: Min/max values
   - **Error cases**: Invalid inputs
   - **Null/undefined tests**: Missing data
   - **API error tests**: Network failures
   - **SmartResponse tests**: Format validation

3. **Create Test File**
   - Setup/teardown functions
   - Mock API responses
   - Test suite structure
   - Assertion helpers

4. **Execute Tests**
   - Run all test cases
   - Measure coverage
   - Check for flaky tests
   - Generate report

**Test Template**:
```typescript
import { describe, it, expect, beforeEach, vi } from 'vitest';
import { toolName } from '../src/tools/toolFile';

describe('toolName', () => {
  // Mock API responses
  const mockSuccessResponse = { /* ... */ };
  const mockErrorResponse = { /* ... */ };

  beforeEach(() => {
    // Reset mocks before each test
    vi.clearAllMocks();
  });

  describe('Happy Path', () => {
    it('should return valid SmartResponse for valid input', async () => {
      const result = await toolName({ symbol: 'ADVANC' });
      expect(result).toMatchSchema(SmartResponseV2);
      expect(result.summary.title).toContain('ADVANC');
    });

    it('should include semantic keyFindings', async () => {
      const result = await toolName({ symbol: 'KBANK' });
      expect(result.summary.keyFindings).toBeArray();
      expect(result.summary.keyFindings[0]).toHaveProperty('metric');
      expect(result.summary.keyFindings[0]).toHaveProperty('assessment');
    });
  });

  describe('Edge Cases', () => {
    it('should handle lowercase symbols', async () => {
      const result = await toolName({ symbol: 'advanc' });
      expect(result.data.symbol).toBe('ADVANC.BK');
    });

    it('should handle symbol with .BK suffix', async () => {
      const result = await toolName({ symbol: 'ADVANC.BK' });
      expect(result.data.symbol).toBe('ADVANC.BK');
    });

    it('should handle missing optional fields', async () => {
      const result = await toolName({ symbol: 'NODATA' });
      expect(result.metadata.warnings).not.toBeEmpty();
      expect(result.summary.confidence.score).toBeLessThan(0.5);
    });
  });

  describe('Error Handling', () => {
    it('should throw error for empty symbol', async () => {
      await expect(toolName({ symbol: '' }))
        .rejects.toThrow('Symbol must be a non-empty string');
    });

    it('should throw error for invalid symbol format', async () => {
      await expect(toolName({ symbol: '123' }))
        .rejects.toThrow('Invalid symbol format');
    });

    it('should handle API timeout', async () => {
      vi.mocked(axios.get).mockImplementation(() =>
        new Promise((resolve) => setTimeout(resolve, 10000))
      );

      await expect(toolName({ symbol: 'TIMEOUT' }))
        .rejects.toThrow('timeout');
    });
  });

  describe('SmartResponse Validation', () => {
    it('should have all required sections', async () => {
      const result = await toolName({ symbol: 'PTT' });
      expect(result).toHaveProperty('summary');
      expect(result).toHaveProperty('data');
      expect(result).toHaveProperty('metadata');
      expect(result).toHaveProperty('recommendations');
      expect(result).toHaveProperty('context');
    });

    it('should have valid confidence score', async () => {
      const result = await toolName({ symbol: 'AOT' });
      expect(result.summary.confidence.score).toBeGreaterThanOrEqual(0);
      expect(result.summary.confidence.score).toBeLessThanOrEqual(1);
      expect(result.summary.confidence.level).toBeOneOf([
        'Very High', 'High', 'Medium', 'Low', 'Very Low'
      ]);
    });
  });
});
```

---

### Workflow 3: API Integration Testing

**Trigger**: "Test API integration for [endpoint/tool]"

**Steps**:
1. **API Analysis**
   - Identify API endpoint
   - Document request format
   - List expected responses
   - Check authentication needs

2. **Test Scenarios**
   - **Success response**: Valid request
   - **Not found**: Invalid symbol
   - **Timeout**: Slow response
   - **Server error**: 500 status
   - **Rate limit**: Too many requests
   - **Malformed response**: Unexpected format

3. **Execute Tests**
   - Test against real API
   - Test with mock server
   - Measure response times
   - Check error handling

4. **Validate Responses**
   - Check data types
   - Validate required fields
   - Test null handling
   - Verify calculations

**API Test Template**:
```typescript
describe('SET Watch API Integration', () => {
  const API_BASE = 'https://set-watch-api.vercel.app';

  describe('fetch_stock_data endpoint', () => {
    it('should fetch valid stock data for ADVANC', async () => {
      const response = await axios.get(`${API_BASE}/mypick/snapStatistics/ADVANC.BK`);

      expect(response.status).toBe(200);
      expect(response.data).toHaveProperty('peRatio');
      expect(response.data).toHaveProperty('eps');
      expect(response.data.peRatio).toBeGreaterThan(0);
    });

    it('should return 404 for invalid symbol', async () => {
      await expect(
        axios.get(`${API_BASE}/mypick/snapStatistics/INVALID.BK`)
      ).rejects.toMatchObject({
        response: { status: 404 }
      });
    });

    it('should handle timeout for slow responses', async () => {
      const controller = new AbortController();
      setTimeout(() => controller.abort(), 100);

      await expect(
        axios.get(`${API_BASE}/mypick/snapStatistics/SLOW.BK`, {
          signal: controller.signal
        })
      ).rejects.toThrow();
    });

    it('should have response time under 3 seconds', async () => {
      const start = Date.now();
      await axios.get(`${API_BASE}/mypick/snapStatistics/BBL.BK`);
      const duration = Date.now() - start;

      expect(duration).toBeLessThan(3000);
    });
  });
});
```

---

### Workflow 4: Regression Testing

**Trigger**: "Run regression tests after [change]"

**Steps**:
1. **Load Baseline**
   - Read baseline results
   - Load previous SmartResponse examples
   - Get expected output format

2. **Compare Results**
   - Run current version
   - Compare with baseline
   - Identify breaking changes
   - Check for new warnings

3. **Validate Compatibility**
   - Test with old input formats
   - Ensure backward compatibility
   - Check API contracts
   - Verify type definitions

4. **Report Regressions**
   - List breaking changes
   - Highlight improvements
   - Suggest fixes
   - Update baseline if intentional

**Regression Test Script**:
```typescript
import fs from 'fs';

interface Baseline {
  tool: string;
  version: string;
  responses: Record<string, any>;
}

async function runRegressionTests(baselineFile: string) {
  const baseline: Baseline = JSON.parse(fs.readFileSync(baselineFile, 'utf-8'));

  const regressions: Array<{ tool: string; input: any; expected: any; actual: any }> = [];
  const improvements: Array<{ tool: string; description: string }> = [];

  for (const [testName, expectedResponse] of Object.entries(baseline.responses)) {
    // Run current version
    const actualResponse = await runTool(testName);

    // Compare
    const diff = compareResponses(expectedResponse, actualResponse);

    if (diff.breakingChanges.length > 0) {
      regressions.push({
        tool: baseline.tool,
        input: testName,
        expected: expectedResponse,
        actual: actualResponse
      });
    }

    if (diff.improvements.length > 0) {
      improvements.push({
        tool: baseline.tool,
        description: diff.improvements.join(', ')
      });
    }
  }

  return { regressions, improvements };
}
```

---

### Workflow 5: Performance Testing

**Trigger**: "Run performance tests for [tool]"

**Metrics to Measure**:
- **Execution time**: Time to complete request
- **Memory usage**: Peak memory during execution
- **API response time**: Time for external API calls
- **Throughput**: Requests per second
- **Resource usage**: CPU, memory, network

**Performance Test Template**:
```typescript
import { performance } from 'perf_hooks';

describe('Performance Tests', () => {
  it('should complete fetch_stock_data in under 3 seconds', async () => {
    const start = performance.now();
    await fetchStockData('ADVANC');
    const duration = performance.now() - start;

    expect(duration).toBeLessThan(3000);
  });

  it('should handle 10 concurrent requests without degradation', async () => {
    const symbols = Array(10).fill('ADVANC');
    const start = performance.now();

    const results = await Promise.all(
      symbols.map(s => fetchStockData(s))
    );

    const duration = performance.now() - start;
    const avgTime = duration / results.length;

    expect(avgTime).toBeLessThan(3500); // Allow 15% slowdown for concurrency
  });

  it('should not leak memory across 100 iterations', async () => {
    const initialMemory = process.memoryUsage().heapUsed;

    for (let i = 0; i < 100; i++) {
      await fetchStockData('KBANK');
    }

    global.gc(); // Force garbage collection if --expose-gc enabled

    const finalMemory = process.memoryUsage().heapUsed;
    const memoryIncrease = finalMemory - initialMemory;

    // Allow 10MB increase for 100 iterations
    expect(memoryIncrease).toBeLessThan(10 * 1024 * 1024);
  });
});
```

---

### Workflow 6: Edge Case Generation

**Trigger**: "Generate edge cases for [function/tool]"

**Edge Case Categories**:

1. **Boundary Values**
   - Minimum/maximum values
   - Zero, negative numbers
   - Empty strings, arrays
   - Null/undefined values

2. **Type Edge Cases**
   - Wrong data types
   - Mixed types in arrays
   - Invalid enum values
   - Malformed objects

3. **API Edge Cases**
   - Network timeouts
   - Empty responses
   - Malformed JSON
   - Missing fields

4. **Business Logic Edge Cases**
   - Zero/negative financial metrics
   - Missing historical data
   - No dividend data
   - Infinite/NaN calculations

**Edge Case Test Generator**:
```typescript
function generateEdgeCases(functionSignature: string) {
  return {
    numeric: [
      { name: 'Zero', value: 0 },
      { name: 'Negative', value: -1 },
      { name: 'Decimal', value: 0.5 },
      { name: 'Very Large', value: Number.MAX_SAFE_INTEGER },
      { name: 'Very Small', value: Number.MIN_SAFE_INTEGER },
      { name: 'NaN', value: NaN },
      { name: 'Infinity', value: Infinity }
    ],

    string: [
      { name: 'Empty', value: '' },
      { name: 'Spaces', value: '   ' },
      { name: 'Special chars', value: '!@#$%^&*()' },
      { name: 'Unicode', value: '‡∏ó‡∏î‡∏™‡∏≠‡∏ö' },
      { name: 'Very long', value: 'a'.repeat(10000) },
      { name: 'Null character', value: '\0' }
    ],

    symbol: [
      { name: 'Lowercase', value: 'advanc' },
      { name: 'With .BK', value: 'ADVANC.BK' },
      { name: 'Too short', value: 'A' },
      { name: 'Too long', value: 'ABCDEFG' },
      { name: 'With numbers', value: 'ADV123' },
      { name: 'With spaces', value: 'ADV ANC' }
    ]
  };
}

// Usage
const edgeCases = generateEdgeCases('calculate_pe_band');

edgeCases.symbol.forEach(testCase => {
  it(`should handle ${testCase.name} symbol`, async () => {
    const result = await calculatePEBand({
      symbol: testCase.value,
      // ... other params
    });

    if (testCase.name === 'Too short' || testCase.name === 'Too long') {
      expect(result).toThrow();
    } else {
      expect(result).toBeValidSmartResponse();
    }
  });
});
```

---

### Workflow 7: Code Coverage Analysis

**Trigger**: "Analyze test coverage for [module/tool]"

**Steps**:
1. **Run Coverage Report**
   ```bash
   npm run test -- --coverage
   ```

2. **Analyze Results**
   - Identify uncovered files
   - Find uncovered branches
   - Check missing lines
   - Review function coverage

3. **Generate Missing Tests**
   - Create tests for uncovered code
   - Add edge case tests
   - Test error paths
   - Cover all branches

4. **Target Improvement**
   - Set coverage targets
   - Track improvement over time
   - Report on progress

**Coverage Report Template**:
```typescript
describe('Coverage Analysis', () => {
  it('should have at least 80% coverage', () => {
    const coverage = getCoverageReport();

    expect(coverage.lines.pct).toBeGreaterThanOrEqual(80);
    expect(coverage.branches.pct).toBeGreaterThanOrEqual(80);
    expect(coverage.functions.pct).toBeGreaterThanOrEqual(80);
    expect(coverage.statements.pct).toBeGreaterThanOrEqual(80);
  });

  describe('Uncovered Code', () => {
    it('should identify uncovered functions', () => {
      const uncovered = getUncoveredFunctions();
      const requiredTests = uncovered.map(fn => ({
        function: fn,
        reason: 'No tests found',
        priority: 'high'
      }));

      console.log('Tests needed for:', requiredTests);
      expect(uncovered.length).toBe(0);
    });
  });
});
```

---

## üìä Quality Gates

### Pre-Merge Checklist

Before merging any code, QABot verifies:

```yaml
quality_gates:
  - name: "TypeScript Compilation"
    check: "npm run build"
    pass_criteria: "No errors"
    severity: "blocking"

  - name: "Type Checking"
    check: "npx tsc --noEmit"
    pass_criteria: "Zero type errors"
    severity: "blocking"

  - name: "Linting"
    check: "npm run lint"
    pass_criteria: "Zero errors"
    severity: "blocking"

  - name: "Unit Tests"
    check: "npm run test"
    pass_criteria: "All tests pass"
    severity: "blocking"

  - name: "Test Coverage"
    check: "npm run test -- --coverage"
    pass_criteria: "‚â• 80% coverage"
    severity: "warning"

  - name: "SmartResponse Validation"
    check: "npm run test -- validate-smartresponse"
    pass_criteria: "100% valid"
    severity: "blocking"

  - name: "Security Scan"
    check: "npm audit"
    pass_criteria: "No high/critical vulnerabilities"
    severity: "blocking"

  - name: "Dependency Check"
    check: "npm outdated"
    pass_criteria: "No major updates behind"
    severity: "warning"
```

### Test Categories

| Category | Description | Priority | Coverage Target |
|----------|-------------|----------|-----------------|
| **Unit** | Individual functions | High | 90% |
| **Integration** | API interactions | High | 80% |
| **E2E** | Complete workflows | Medium | 70% |
| **Performance** | Speed, memory | Medium | N/A |
| **Security** | Vulnerabilities | High | 100% |
| **SmartResponse** | Format validation | High | 100% |

---

## üîç Bug Detection Patterns

### Common Bug Patterns

1. **Null Reference Errors**
   ```typescript
   // Bug
   const price = data.price.toFixed(2);  // Crashes if data.price is null

   // Fix
   const price = data.price?.toFixed(2) || 'N/A';
   ```

2. **Missing Error Handling**
   ```typescript
   // Bug
   const result = await fetchData(symbol);
   return result.data;  // No error handling

   // Fix
   try {
     const result = await fetchData(symbol);
     return result.data;
   } catch (error) {
     return { error: error.message };
   }
   ```

3. **Type Coercion Issues**
   ```typescript
   // Bug
   if (data.peRatio == 0) { }  // Matches null, undefined, '0'

   // Fix
   if (data.peRatio === 0) { }  // Exact match
   ```

4. **Array Index Out of Bounds**
   ```typescript
   // Bug
   const latest = data[0];  // Crashes if array empty

   // Fix
   const latest = data[0] ?? null;
   ```

5. **Async/Await Mistakes**
   ```typescript
   // Bug
   const data = fetchData(symbol);  // Returns promise, not data
   return data.price;

   // Fix
   const data = await fetchData(symbol);
   return data.price;
   ```

### Bug Detection Checklist

- [ ] All null/undefined checks in place
- [ ] All async functions properly awaited
- [ ] All array accesses have bounds checking
- [ ] All type conversions are explicit
- [ ] All error cases are handled
- [ ] All API responses are validated
- [ ] All calculations handle edge cases (zero, negative, infinity)
- [ ] All dates are properly formatted
- [ ] All user inputs are validated
- [ ] All external calls have timeouts

---

## üìà Test Coverage Strategy

### Coverage by Module

```
src/
‚îú‚îÄ‚îÄ config/index.ts          [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 95%  Critical - API config
‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts             [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% Critical - Core types
‚îÇ   ‚îú‚îÄ‚îÄ responses.ts         [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë] 85%  High - SmartResponse
‚îÇ   ‚îî‚îÄ‚îÄ canslim.ts           [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 90%  High - CANSLIM types
‚îî‚îÄ‚îÄ tools/
    ‚îú‚îÄ‚îÄ setWatchApi.ts       [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 75%  High - Core data fetching
    ‚îú‚îÄ‚îÄ stockValuation.ts    [‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 60%  Medium - Valuation models
    ‚îú‚îÄ‚îÄ financialStatements.ts [‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 50%  Low - Needs more tests
    ‚îú‚îÄ‚îÄ canslim.ts           [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 70%  Medium - CANSLIM
    ‚îî‚îÄ‚îÄ webTools.ts          [‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 65%  Low - Web tools

Overall Coverage: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 72%  Target: 80%
```

### Coverage Goals

| Module | Current | Target | Priority |
|--------|---------|--------|----------|
| Core types | 100% | 100% | ‚úÖ Met |
| SmartResponse | 85% | 95% | High |
| Data fetching | 75% | 90% | High |
| Valuation | 60% | 85% | High |
| Financial statements | 50% | 80% | Medium |
| Web tools | 65% | 85% | Medium |

---

## üéØ Quality Metrics Dashboard

### Current Status

```yaml
test_summary:
  total_tests: 156
  passing: 148
  failing: 5
  skipped: 3
  pass_rate: "94.9%"

coverage:
  lines: "72.4%"
  functions: "68.9%"
  branches: "65.2%"
  statements: "74.1%"

quality_metrics:
  lint_errors: 0
  type_errors: 0
  security_issues: 0
  code_smells: 12
  technical_debt: "Medium"

smartresponse_validation:
  total_tools: 13
  v2_compliant: 3
  v1_compliant: 10
  validation_rate: "23%"
```

### Target Goals

```yaml
goals:
  test_pass_rate: "‚â• 99%"
  coverage_lines: "‚â• 80%"
  coverage_branches: "‚â• 75%"
  lint_errors: 0
  type_errors: 0
  smartresponse_v2: "100% by end of Phase 2"
```

---

## üö® Alert Thresholds

### Alert Levels

| Level | Condition | Action |
|-------|-----------|--------|
| üî¥ Critical | Test failure rate > 10% | Block deployment |
| üü† Warning | Coverage drops > 5% | Notify team |
| üü° Info | New code smells detected | Create ticket |
| üîµ Success | All gates passing | Allow merge |

### Automated Alerts

```typescript
interface AlertRule {
  name: string;
  condition: string;
  severity: 'critical' | 'warning' | 'info';
  action: string;
}

const alertRules: AlertRule[] = [
  {
    name: 'Test Failures',
    condition: 'failing_tests > 5',
    severity: 'critical',
    action: 'Block merge and notify team'
  },
  {
    name: 'Coverage Drop',
    condition: 'coverage < 70%',
    severity: 'warning',
    action: 'Create coverage improvement ticket'
  },
  {
    name: 'Type Errors',
    condition: 'type_errors > 0',
    severity: 'critical',
    action: 'Block merge immediately'
  },
  {
    name: 'SmartResponse Validation',
    condition: 'invalid_smartresponse > 0',
    severity: 'warning',
    action: 'Create validation fix ticket'
  }
];
```

---

## üìù Test Report Template

### Daily Test Report

```markdown
# Test Report - 2025-01-17

## Summary
- **Total Tests**: 156
- **Passing**: 148 (94.9%)
- **Failing**: 5
- **Skipped**: 3
- **Duration**: 45.2s

## Coverage
- Lines: 72.4% (target: 80%)
- Functions: 68.9% (target: 80%)
- Branches: 65.2% (target: 75%)

## Failing Tests
1. `calculate_dcf handles zero FCF`
2. `fetch_stock_data handles timeout` (flaky)
3. `smartresponse v2 has semantic keyFindings`
4. `margin_of_safety calculates correctly for negative MOS`
5. `web_search handles network errors` (flaky)

## Issues Found
- [BUG] calculate_dcf crashes when FCF is zero
- [FLAKY] fetch_stock_data timeout test intermittently fails
- [TODO] Convert 10 tools to SmartResponseV2

## Recommendations
1. Fix zero FCF handling in calculate_dcf
2. Investigate flaky tests (increase timeout?)
3. Continue SmartResponseV2 migration (3/13 complete)

## Next Steps
- [ ] Fix calculate_dcf zero FCF bug
- [ ] Increase API timeout in test config
- [ ] Convert next 3 tools to V2
```

---

## üîÑ CI/CD Integration

### GitHub Actions Workflow

```yaml
name: QA Pipeline

on: [push, pull_request]

jobs:
  qa:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '21'

      - name: Install dependencies
        run: npm ci

      - name: Type check
        run: npx tsc --noEmit

      - name: Lint
        run: npm run lint

      - name: Unit tests
        run: npm run test -- --coverage

      - name: Validate SmartResponse
        run: npm run test -- validate-smartresponse

      - name: Security audit
        run: npm audit --audit-level=high

      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

---

## üìû Agent Handoff Protocol

### When to Escalate

- Architecture changes affecting test strategy
- Performance issues requiring profiling
- Security vulnerabilities needing review
- Test environment failures
- Blocking bugs preventing deployment

### Success Metrics

- **Test Pass Rate**: ‚â• 99%
- **Coverage**: ‚â• 80%
- **SmartResponse Valid**: 100%
- **Zero Critical Bugs** in production
- **Average Test Duration**: < 5 minutes

---

**Agent Version**: 1.0
**Last Updated**: 2025-01-17
**Maintainer**: QA Team
